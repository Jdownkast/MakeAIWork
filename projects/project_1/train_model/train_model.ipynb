{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "048aee69-592b-49d3-aa4b-c9b86cd54e37",
   "metadata": {},
   "source": [
    "# This notebook describes the training of the datamodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c29e23b-5ef2-4cb9-ae4e-3f5365d648d7",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b90251df-210a-4f70-b346-7f755bb8e63e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import f_regression\n",
    "import statsmodels.api as sm\n",
    "from yellowbrick.regressor import ResidualsPlot\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a732168-601f-4fac-a63f-55d5100f34e8",
   "metadata": {},
   "source": [
    "**Naming dictionary**<br>\n",
    "This directory is used to easily change the column names of the DataFrames. I made it a dictionary, because I suspected that when I defined the function 'lr_dict' this function would not have access to the variables that are created under \"Unpack dictionary...\" (see next codeblock). However, the variables seem to be global (?) and not need any further unpacking, thus making creating the dictionary a bit unnecessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4220d50c-1ff6-4fc0-986a-3c64ab87f845",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set a naming dictionary, to easily change names\n",
    "naming = {\n",
    "    'm_name': \"Model_\",\n",
    "    'all_data': 'Data',\n",
    "    'x_data': 'Independend data',\n",
    "    'y_data': 'Dependend data',\n",
    "    'x_train': 'Independend train data',\n",
    "    'y_train': 'Dependend train data',\n",
    "    'x_test': 'Independend test data',\n",
    "    'y_test': 'Dependend test data',\n",
    "    'y_pred': 'Predicted data',\n",
    "    'model': 'Model'\n",
    "}\n",
    "\n",
    "# Unpack dictionary to use in script\n",
    "for key, name in naming.items():\n",
    "        exec(key + '=name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aca1ab5-8425-40fd-ac34-7a606dc8bfd6",
   "metadata": {},
   "source": [
    "**Load trainingsets into dictionairy**<br>\n",
    "Below an overview of all the datasets that (in principle) are available from the pipeline. However, further in the project we might decide to not make all these sets available, but only the sets we decide to use.\n",
    "\n",
    "The sets we use to create our linear regression models are loaded into an initial dictionary. I choose a dictionary with slices of the DataFrame, instead of a dataframe itself, so I could later easily 'call' the different slices I needed. I feel that for a dataset this size, this would be a viable option. However, if the dataset would be larger, this method might take too much memory."
   ]
  },
  {
   "cell_type": "raw",
   "id": "368680b1-e0b0-4049-b866-3a16c69f7a36",
   "metadata": {},
   "source": [
    "Model overview (copy from pipeline: 'tform_db')\n",
    "    0: # Complete dataset\n",
    "    1: # All continuous/ratio ('main') variables\n",
    "    2: # All main variables except length & mass\n",
    "    3: # M1, but bmi normalized\n",
    "    4: # M1, but bmi categorized\n",
    "    5: # M1, but smoking categorized\n",
    "    6: # M3, but smoking categorized\n",
    "    7: # M4, but smoking categorized\n",
    "    8: # M2, but bmi normalized\n",
    "    9: # M2, but bmi categorized\n",
    "    10: # M2, but smoking categorized\n",
    "    11: # M8, but smoking categorized\n",
    "    12: # M9, but smoking categorized\n",
    "    13: # All main variables except genetics\n",
    "    14: # All main variables except genetics, length & mass\n",
    "    15: # M13, but bmi normalized\n",
    "    16: # M13, but bmi categorized\n",
    "    17: # M13, but smoking categorized\n",
    "    18: # M14, but smoking categorized\n",
    "    19: # M15, but smoking categorized\n",
    "    20: # M3, but without genetics\n",
    "    21: # M1, but without length, mass & bmi\n",
    "    22: # M3, but without exercise\n",
    "    23: # M3, but without smoking\n",
    "    24: # M3, but without alcohol\n",
    "    25: # M3, but without sugar\n",
    "    26: # M3, but without smoking & alcohol\n",
    "    27: # M3, but without smoking, alcohol & sugar\n",
    "    28: # M3, but without exercise, smoking alcohol & sugar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5139915-d247-49ba-8a35-494d76e040d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 20, 21, 22, 23, 24, 25, 26, 27, 28]\n"
     ]
    }
   ],
   "source": [
    "# Select datasets to be modelled\n",
    "datasets = [1, 3, *range(20, 29)] # All datasets (except 0)\n",
    "print(datasets)\n",
    "\n",
    "# Initialize dictionary and load datasets\n",
    "initial_dict = {}\n",
    "path_datasets = \"../tform_db/csv/\"\n",
    "\n",
    "for dataset in datasets:\n",
    "    model_name = m_name + str(dataset)\n",
    "    file = f\"{path_datasets}db_t{str(dataset)}.csv\"\n",
    "    initial_dict[model_name] = {all_data: pd.read_csv(file)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646aed9d-bddf-444e-a90c-01d68340f4c1",
   "metadata": {},
   "source": [
    "*If necessary, check the datasets that are loaded* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f15b4083-faed-45ae-b227-1269466cb951",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set true if you want to inspect the datasets after loading\n",
    "if False:\n",
    "    for num, models in enumerate(initial_dict):\n",
    "        print(num + 1, initial_dict[models]['Data'].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df346a4-a88e-4941-845e-2bf531c75b64",
   "metadata": {},
   "source": [
    "**Predifine function to clear all files in directory**<br>\n",
    "*Watch out with this function for deleting the wrong files!* \n",
    "\n",
    "This function enables to directory 'models' to be as clean as possible (so it contains only the models that are called in this script)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5eec44f-053a-4576-a2a1-f64347351130",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove files in a specific path with a specific extension\n",
    "def remove_files(path, ext):\n",
    "    for f in os.listdir(path):\n",
    "        if ext in f:\n",
    "            os.remove(os.path.join(path, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c87b39c-33bb-4b13-863a-bbf7e55a2e23",
   "metadata": {},
   "source": [
    "**Predefine regression function**<br>\n",
    "I have chosen to 'experiment' with functions to automatically create the slices of the dataset and put them in a dictionary. As said above, this might be inefficient in regard to memory use, however for a dataset this size it is not (yet) a problem. \n",
    "\n",
    "Furthermore I experimented with handling all the different datasets at the same time and putting them in the same dictionary. This too can be consideren memory-inefficient, still the dictionary seems to be small enough to be handled fast. \n",
    "\n",
    "The function then models (linear regression) the different datasets and saves the result in the same dataset. This model will later be summarized and saved. Linear regression seems applicable for this type of analysis. Other types (K-nearest, decision tree, random forest, ...) were briefly considered but do not seem to bring extra predictive power to the models, as well as I do not (yet) understand the right application of these methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d16f5ed-5d9e-46d8-89d9-bbce34152419",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining the linear regression model\n",
    "def lr_dict(initial_dict):\n",
    "    # Unpack naming dictionary to use in function\n",
    "    # for key, name in naming.items():\n",
    "    #     exec(key + '=name')\n",
    "\n",
    "    for mod in initial_dict:\n",
    "        # Per model, split (in)dependend data\n",
    "        initial_dict[mod][x_data] = initial_dict[mod][all_data].iloc[:, 1:]\n",
    "        initial_dict[mod][y_data] = initial_dict[mod][all_data].iloc[:, :1]\n",
    "\n",
    "        # Per model, split train-/test-data\n",
    "        x_train_d, x_test_d, y_train_d, y_test_d = train_test_split(\n",
    "            initial_dict[mod][x_data], \n",
    "            initial_dict[mod][y_data], \n",
    "            test_size=0.7, random_state=42)\n",
    "        \n",
    "        initial_dict[mod][x_train] = x_train_d\n",
    "        initial_dict[mod][y_train] = y_train_d\n",
    "        initial_dict[mod][x_test] = x_test_d\n",
    "        initial_dict[mod][y_test] = y_test_d\n",
    "        \n",
    "        # Fit model\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(initial_dict[mod][x_train], \n",
    "                  initial_dict[mod][y_train])\n",
    "\n",
    "        # Predict and save data\n",
    "        initial_dict[mod][y_pred] = lr.predict(initial_dict[mod][x_test])\n",
    "        initial_dict[mod][model] = lr\n",
    "    \n",
    "    # Return the new dictionary\n",
    "    return initial_dict\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268c44a5-f68d-4e55-852a-36608c65afc2",
   "metadata": {},
   "source": [
    "**Predefine summary function**<br>\n",
    "This function takes the dictionary with multiple models and summarized the contents of the models. There are two levels of summaries to be given: extended (includes variable names, coefficients and p-values) and limited (R2 and SME/RSME only). If desired, the summary can be saved for further analysis.\n",
    "\n",
    "For the p-values I had to use statsmodel, which essentially does the same as scikit-learn. However, for the purpose of studying scikit-learn, I kept using this package for further modelling and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b29a1138-62ad-4434-9c26-98444bef80b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function to summarize the models\n",
    "def model_summary(models_dict, level ='extended'):\n",
    "    # Preloading variables/summary\n",
    "    var = \"Variable\"\n",
    "    coef = \"Coefficient\"\n",
    "    sig = \"P-value\"\n",
    "    col_names = [var, coef, sig]\n",
    "    \n",
    "    param = \"Parameters\"\n",
    "    metric = \"Metrics\"\n",
    "    summary = {}\n",
    "    \n",
    "    for mod in models_dict:\n",
    "        # Per model, get coefficients\n",
    "        summary[mod] = mod\n",
    "        df_int = pd.DataFrame({var: \"Intercept\", coef: models_dict[mod][model].intercept_, sig: 0})\n",
    "        df_coef = pd.DataFrame(zip(models_dict[mod][x_test].columns, models_dict[mod][model].coef_[0], \n",
    "                                   np.zeros(len(models_dict[mod][x_test]))), columns=col_names)\n",
    "        summary[mod] = {}\n",
    "        summary[mod][param] = pd.concat([df_int, df_coef], axis=0, ignore_index=True)\n",
    "\n",
    "        # Per model, get p-values (inefficiently done with statsmodels, but...)\n",
    "        x_train_sm = models_dict[mod][x_train].copy()\n",
    "        x_train_sm.insert(0, 'intercept', np.ones(len(x_train_sm)))\n",
    "        modsm = sm.OLS(models_dict[mod][y_train], x_train_sm.astype(float))\n",
    "        modsm = modsm.fit()\n",
    "        summary[mod][param][sig] = modsm.pvalues.values.round(3)\n",
    "\n",
    "        # Per model, get the regression metrics\n",
    "        summary[mod][metric] = {}\n",
    "        summary[mod][metric]['r2'] = metrics.r2_score(models_dict[mod][y_test], \n",
    "                                              models_dict[mod][y_pred])\n",
    "        summary[mod][metric]['mse'] = metrics.mean_squared_error(models_dict[mod][y_test], \n",
    "                                         models_dict[mod][y_pred]) \n",
    "        summary[mod][metric]['rmse'] = np.sqrt(summary[mod][metric]['mse'])\n",
    "        summary[mod][metric]['msle'] = metrics.mean_squared_log_error(models_dict[mod][y_test], \n",
    "                                                                models_dict[mod][y_pred])\n",
    "        summary[mod][metric]['mae'] = metrics.mean_absolute_error(models_dict[mod][y_test], \n",
    "                                                          models_dict[mod][y_pred]) \n",
    "            \n",
    "        # Per model, print the summary depending on 'extended' or 'limited' level\n",
    "        if level == 'extended' or level == 'ext':\n",
    "            # pd.set_option(\"display.precision\", 3)\n",
    "            print(mod, \":\")\n",
    "            print(summary[mod][param])\n",
    "            print(\"\\n\")\n",
    "            print('R-squared: ', '%.4f' % round(summary[mod][metric]['r2'], 4), \n",
    "                  '; Mean squared error: ', '%.4f' % round(summary[mod][metric]['mse'] ,4),\n",
    "                  '; Root mean squared error: ', '%.4f' % round(summary[mod][metric]['rmse'] ,4), sep='')\n",
    "            print('___ \\n')\n",
    "            # pd.reset_option(\"display.precision\")\n",
    "        elif level == 'limited' or level == 'lim':\n",
    "            print(mod, \":\", \n",
    "                  ' R-squared: ', '%.4f' % round(summary[mod][metric]['r2'], 4), \n",
    "                  '; Mean squared error: ', '%.4f' % round(summary[mod][metric]['mse'] ,4),\n",
    "                  '; Root mean squared error: ', '%.4f' % round(summary[mod][metric]['rmse'] ,4), sep='')\n",
    "        else:\n",
    "            print(\"Please use level \"\"extended\"\" (\"\"ext\"\") or \"\"limited\"\" (\"\"lim\"\") or omit a choice.\")\n",
    "    \n",
    "    # Return summary to be saved if desired\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f9749b-440e-49dd-a034-8dc9ec6c53e0",
   "metadata": {},
   "source": [
    "### Fitting and predicting\n",
    "Here we use the function we created before to slice, fit and precict the datasets and models. The result is cast in models_dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce2282c8-cdb5-430e-a37b-d5134d08a3ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "# Fit models and save in models_dict\n",
    "models_dict = lr_dict(initial_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebbf4bf-88aa-4a0e-97d7-fee9bd3fb179",
   "metadata": {},
   "source": [
    "### Displaying and comparing model outcomes\n",
    "Here we display the summary of the models. Part of the code can be turned on/off to show the residual plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fea165e-46d8-4f60-8605-19b759ef6683",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_1: R-squared: 0.9809; Mean squared error: 1.1730; Root mean squared error: 1.0831\n",
      "Model_3: R-squared: 0.9816; Mean squared error: 1.1316; Root mean squared error: 1.0638\n",
      "Model_20: R-squared: 0.0799; Mean squared error: 56.5622; Root mean squared error: 7.5208\n",
      "Model_21: R-squared: 0.9734; Mean squared error: 1.6373; Root mean squared error: 1.2796\n",
      "Model_22: R-squared: 0.9677; Mean squared error: 1.9880; Root mean squared error: 1.4100\n",
      "Model_23: R-squared: 0.9267; Mean squared error: 4.5047; Root mean squared error: 2.1224\n",
      "Model_24: R-squared: 0.9790; Mean squared error: 1.2910; Root mean squared error: 1.1362\n",
      "Model_25: R-squared: 0.9810; Mean squared error: 1.1693; Root mean squared error: 1.0813\n",
      "Model_26: R-squared: 0.9243; Mean squared error: 4.6553; Root mean squared error: 2.1576\n",
      "Model_27: R-squared: 0.9237; Mean squared error: 4.6876; Root mean squared error: 2.1651\n",
      "Model_28: R-squared: 0.9097; Mean squared error: 5.5527; Root mean squared error: 2.3564\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show limited/extended summary\n",
    "model_summary(models_dict, 'lim')\n",
    "print() # Otherwise prints return statement (= summary)\n",
    "\n",
    "# If required, show residual plots\n",
    "if False:\n",
    "    for mod in models_dict:\n",
    "        rplot = ResidualsPlot(models_dict[mod][model],\n",
    "                              title=f\"Residuals of {mod}\", \n",
    "                              size=(400, 300))\n",
    "        rplot.fit(models_dict[mod][x_train], models_dict[mod][y_train])\n",
    "        rplot.score(models_dict[mod][x_test], models_dict[mod][y_test])\n",
    "        rplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45391f0d-f4db-43ab-bdee-510577ff9c27",
   "metadata": {},
   "source": [
    "**Interpretation**<br>\n",
    "The higher the R-squared value of a model, the better the model is able to predict the variance of the dependent variable based on the independend variables. The mean (root) mean squared error also indicated the size of the errorterm, where mean squared error is more sensitive for outliers. \n",
    "\n",
    "For the final run, I included 11 models. Two (the 'main contestants', models 1 and 3; see below) because of their high R2 and because they are somewhat different from eachother. Models 20 to 28 are included to give alternative options to the end user: for example if some predictors are not available. For these models the note has to be given that their predictive value is lower or much lower, depending on the missing information. \n",
    "\n",
    "We see that there are two 'main contestants' for a definite model, namely model_1 (all variables) an model_3 (all variables, but normalized bmi). Model_25 (all variables, normalized bmi and without sugar) seems to be another contender, however this only shows that sugar is not a very important factor. If all models would be considered, also model 8 would show to have a large R2. However, as that model is very similar to model 3, and we prefer model 3 for the larger R2, we excluded model 8. \n",
    "\n",
    "The residual plot of the models needs to have normalized distribution. This seems to be the case for most of the models, however models 23, 26 and 27 (possibly 28) seem to be skewed. For now I do not know how to handle this so I will ignore this fact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57cc751-ae92-4883-b00e-58963dcec04c",
   "metadata": {},
   "source": [
    "### Saving model with pickle\n",
    "This concludes the modelling and selection of models. We save to models for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "392d7cd5-14f0-4458-b9b5-b8e12f921624",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Can be turned off\n",
    "if True:\n",
    "    path_models = \"./models/\"\n",
    "    extension = \".pickle\"\n",
    "\n",
    "    # Delete all models in directory\n",
    "    remove_files(path_models, extension)\n",
    "\n",
    "    # Save all new models\n",
    "    for mod in models_dict:\n",
    "        filename = f\"{path_models}{mod}{extension}\"\n",
    "        pickle.dump(models_dict[mod][model], open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46625f1-0a04-4e8e-9747-c6fd9e8c4f7e",
   "metadata": {},
   "source": [
    "**To improve**\n",
    "- WHAT DO THE P-VALUES MEAN??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
